From df631637aa70d73d3f127ee435bda54f15c64606 Mon Sep 17 00:00:00 2001
From: dsamersoff <dms@samersoff.net>
Date: Wed, 9 Jun 2021 11:51:55 +0300
Subject: [PATCH 3/3] 8212657 Fixed DMS

---
 src/hotspot/share/gc/g1/g1CollectedHeap.cpp   |  3 ++
 src/hotspot/share/gc/g1/g1Policy.cpp          | 52 ++++++++++++++-----
 .../gc/g1/g1YoungRemSetSamplingThread.cpp     | 48 ++++++++++++++++-
 .../gc/g1/g1YoungRemSetSamplingThread.hpp     |  8 ++-
 src/hotspot/share/gc/g1/g1_globals.hpp        | 19 ++++++-
 src/hotspot/share/gc/shared/gcCause.cpp       |  3 ++
 src/hotspot/share/gc/shared/gcCause.hpp       |  1 +
 .../share/gc/shared/vmGCOperations.cpp        |  2 -
 src/hotspot/share/logging/logTag.hpp          |  1 +
 9 files changed, 118 insertions(+), 19 deletions(-)

diff --git a/src/hotspot/share/gc/g1/g1CollectedHeap.cpp b/src/hotspot/share/gc/g1/g1CollectedHeap.cpp
index 4c51c5a474..5924b108c8 100644
--- a/src/hotspot/share/gc/g1/g1CollectedHeap.cpp
+++ b/src/hotspot/share/gc/g1/g1CollectedHeap.cpp
@@ -1153,6 +1153,8 @@ void G1CollectedHeap::do_full_collection(bool clear_all_soft_refs) {
 }
 
 void G1CollectedHeap::resize_heap_if_necessary() {
+  assert_at_safepoint_on_vm_thread();
+
   // Capacity, free and used after the GC counted as full regions to
   // include the waste in the following calculations.
   const size_t capacity_after_gc = capacity();
@@ -1916,6 +1918,7 @@ bool G1CollectedHeap::should_do_concurrent_full_gc(GCCause::Cause cause) {
   switch (cause) {
     case GCCause::_gc_locker:               return GCLockerInvokesConcurrent;
     case GCCause::_g1_humongous_allocation: return true;
+    case GCCause::_g1_periodic_collection:  return G1PeriodicGCInvokesConcurrent;
     default:                                return is_user_requested_concurrent_full_gc(cause);
   }
 }
diff --git a/src/hotspot/share/gc/g1/g1Policy.cpp b/src/hotspot/share/gc/g1/g1Policy.cpp
index 7096b62564..26ba116144 100644
--- a/src/hotspot/share/gc/g1/g1Policy.cpp
+++ b/src/hotspot/share/gc/g1/g1Policy.cpp
@@ -703,19 +703,43 @@ void G1Policy::record_collection_pause_end(double pause_time_ms, size_t cards_sc
   }
 
   _free_regions_at_end_of_collection = _g1h->num_free_regions();
-  // IHOP control wants to know the expected young gen length if it were not
-  // restrained by the heap reserve. Using the actual length would make the
-  // prediction too small and the limit the young gen every time we get to the
-  // predicted target occupancy.
-  size_t last_unrestrained_young_length = update_young_list_max_and_target_length();
-  update_rs_lengths_prediction();
-
-  _old_gen_alloc_tracker.reset_after_gc(_g1h->humongous_regions_count() * HeapRegion::GrainBytes);
-  update_ihop_prediction(app_time_ms / 1000.0,
-                         last_unrestrained_young_length * HeapRegion::GrainBytes,
-                         this_pause_was_young_only);
 
-  _ihop_control->send_trace_event(_g1h->gc_tracer_stw());
+  // DMS START
+  update_rs_lengths_prediction();
+  _old_gen_alloc_tracker.reset_after_gc(_g1h->humongous_regions_count() * HeapRegion::GrainBytes); // DMS CHECK
+
+  // Do not update dynamic IHOP due to G1 periodic collection as it is highly likely
+  // that in this case we are not running in a "normal" operating mode.
+  if (_g1h->gc_cause() != GCCause::_g1_periodic_collection) {
+    // IHOP control wants to know the expected young gen length if it were not
+    // restrained by the heap reserve. Using the actual length would make the
+    // prediction too small and the limit the young gen every time we get to the
+    // predicted target occupancy.
+    size_t last_unrestrained_young_length = update_young_list_max_and_target_length();
+
+    update_ihop_prediction(app_time_ms / 1000.0,
+                           last_unrestrained_young_length * HeapRegion::GrainBytes,
+		           this_pause_was_young_only);
+
+
+//    update_ihop_prediction(app_time_ms / 1000.0,
+//                           _bytes_allocated_in_old_since_last_gc,
+//                           last_unrestrained_young_length * HeapRegion::GrainBytes,
+//                           this_pause_was_young_only);
+//    _bytes_allocated_in_old_since_last_gc = 0;
+
+    _ihop_control->send_trace_event(_g1h->gc_tracer_stw());
+
+   } else {
+    // Any garbage collection triggered as periodic collection resets the time-to-mixed
+    // measurement. Periodic collection typically means that the application is "inactive", i.e.
+    // the marking threads may have received an uncharacterisic amount of cpu time
+    // for completing the marking, i.e. are faster than expected.
+    // This skews the predicted marking length towards smaller values which might cause
+    // the mark start being too late.
+    _initial_mark_to_mixed.reset();
+  }
+  //DMS END
 
   // Note that _mmu_tracker->max_gc_time() returns the time in seconds.
   double update_rs_time_goal_ms = _mmu_tracker->max_gc_time() * MILLIUNITS * G1RSetUpdatingPauseTimePercent / 100.0;
@@ -1067,7 +1091,9 @@ void G1Policy::record_pause(PauseKind kind, double start, double end) {
       _initial_mark_to_mixed.add_pause(end - start);
       break;
     case InitialMarkGC:
-      _initial_mark_to_mixed.record_initial_mark_end(end);
+      if (_g1h->gc_cause() != GCCause::_g1_periodic_collection) {
+        _initial_mark_to_mixed.record_initial_mark_end(end);
+      }
       break;
     case MixedGC:
       _initial_mark_to_mixed.record_mixed_gc_start(start);
diff --git a/src/hotspot/share/gc/g1/g1YoungRemSetSamplingThread.cpp b/src/hotspot/share/gc/g1/g1YoungRemSetSamplingThread.cpp
index 7b1133a484..d78c28cf31 100644
--- a/src/hotspot/share/gc/g1/g1YoungRemSetSamplingThread.cpp
+++ b/src/hotspot/share/gc/g1/g1YoungRemSetSamplingThread.cpp
@@ -25,6 +25,8 @@
 #include "precompiled.hpp"
 #include "gc/g1/g1CollectedHeap.inline.hpp"
 #include "gc/g1/g1CollectionSet.hpp"
+#include "gc/g1/g1ConcurrentMark.inline.hpp"
+#include "gc/g1/g1ConcurrentMarkThread.inline.hpp"
 #include "gc/g1/g1Policy.hpp"
 #include "gc/g1/g1YoungRemSetSamplingThread.hpp"
 #include "gc/g1/heapRegion.inline.hpp"
@@ -37,7 +39,8 @@ G1YoungRemSetSamplingThread::G1YoungRemSetSamplingThread() :
     _monitor(Mutex::nonleaf,
              "G1YoungRemSetSamplingThread monitor",
              true,
-             Monitor::_safepoint_check_never) {
+             Monitor::_safepoint_check_never),
+    _last_periodic_gc_attempt_s(os::elapsedTime()) {
   set_name("G1 Young RemSet Sampling");
   create_and_start();
 }
@@ -45,11 +48,50 @@ G1YoungRemSetSamplingThread::G1YoungRemSetSamplingThread() :
 void G1YoungRemSetSamplingThread::sleep_before_next_cycle() {
   MutexLockerEx x(&_monitor, Mutex::_no_safepoint_check_flag);
   if (!should_terminate()) {
-    uintx waitms = G1ConcRefinementServiceIntervalMillis; // 300, really should be?
+    uintx waitms = G1ConcRefinementServiceIntervalMillis;
     _monitor.wait(Mutex::_no_safepoint_check_flag, waitms);
   }
 }
 
+bool G1YoungRemSetSamplingThread::should_start_periodic_gc() {
+  // If we are currently in a concurrent mark we are going to uncommit memory soon.
+  if (G1CollectedHeap::heap()->concurrent_mark()->cm_thread()->during_cycle()) {
+    log_debug(gc, periodic)("Concurrent cycle in progress. Skipping.");
+    return false;
+  }
+
+  // Check if enough time has passed since the last GC.
+  uintx time_since_last_gc = 0;
+  if ((G1PeriodicGCInterval == 0) ||
+      ((time_since_last_gc = (uintx)Universe::heap()->millis_since_last_gc()) < G1PeriodicGCInterval))
+  {
+    log_debug(gc, periodic)("Last GC occurred " UINTX_FORMAT "ms before which is below threshold " UINTX_FORMAT "ms. Skipping.",
+                            time_since_last_gc, G1PeriodicGCInterval);
+    return false;
+  }
+
+  // Check if load is lower than max.
+  double recent_load;
+  if ((G1PeriodicGCSystemLoadThreshold > 0) &&
+      (os::loadavg(&recent_load, 1) == -1 || recent_load > G1PeriodicGCSystemLoadThreshold)) {
+    log_debug(gc, periodic)("Load %1.2f is higher than threshold " UINTX_FORMAT ". Skipping.",
+                            recent_load, G1PeriodicGCSystemLoadThreshold);
+    return false;
+  }
+
+  return true;
+}
+
+void G1YoungRemSetSamplingThread::check_for_periodic_gc(){
+  if ((os::elapsedTime() - _last_periodic_gc_attempt_s) > (G1PeriodicGCInterval / 1000.0)) {
+    log_debug(gc, periodic)("Checking for periodic GC.");
+    if (should_start_periodic_gc()) {
+      Universe::heap()->collect(GCCause::_g1_periodic_collection);
+    }
+    _last_periodic_gc_attempt_s = os::elapsedTime();
+  }
+}
+
 void G1YoungRemSetSamplingThread::run_service() {
   double vtime_start = os::elapsedVTime();
 
@@ -62,6 +104,8 @@ void G1YoungRemSetSamplingThread::run_service() {
       _vtime_accum = 0.0;
     }
 
+    check_for_periodic_gc();
+
     sleep_before_next_cycle();
   }
 }
diff --git a/src/hotspot/share/gc/g1/g1YoungRemSetSamplingThread.hpp b/src/hotspot/share/gc/g1/g1YoungRemSetSamplingThread.hpp
index b5215efbd1..f0068a24db 100644
--- a/src/hotspot/share/gc/g1/g1YoungRemSetSamplingThread.hpp
+++ b/src/hotspot/share/gc/g1/g1YoungRemSetSamplingThread.hpp
@@ -43,14 +43,20 @@ class G1YoungRemSetSamplingThread: public ConcurrentGCThread {
 private:
   Monitor _monitor;
 
+  double _last_periodic_gc_attempt_s;
+
+  double _vtime_accum;  // Accumulated virtual time.
+
   void sample_young_list_rs_lengths();
 
   void run_service();
+  void check_for_periodic_gc();
+
   void stop_service();
 
   void sleep_before_next_cycle();
 
-  double _vtime_accum;  // Accumulated virtual time.
+  bool should_start_periodic_gc();
 
 public:
   G1YoungRemSetSamplingThread();
diff --git a/src/hotspot/share/gc/g1/g1_globals.hpp b/src/hotspot/share/gc/g1/g1_globals.hpp
index 8c7aec8472..82b4c43b88 100644
--- a/src/hotspot/share/gc/g1/g1_globals.hpp
+++ b/src/hotspot/share/gc/g1/g1_globals.hpp
@@ -302,6 +302,23 @@
           "Verify the code root lists attached to each heap region.")       \
                                                                             \
   develop(bool, G1VerifyBitmaps, false,                                     \
-          "Verifies the consistency of the marking bitmaps")
+          "Verifies the consistency of the marking bitmaps")                \
+                                                                            \
+  manageable(uintx, G1PeriodicGCInterval, 0,                                \
+          "Number of milliseconds after a previous GC to wait before "      \
+          "triggering a periodic gc. A value of zero disables periodically "\
+          "enforced gc cycles.")                                            \
+          range(0, max_jlong)                                               \
+                                                                            \
+  product(bool, G1PeriodicGCInvokesConcurrent, true,                        \
+          "Determines the kind of periodic GC. Set to true to have G1 "     \
+          "perform a concurrent GC as periodic GC, otherwise use a STW "    \
+          "Full GC.")                                                       \
+                                                                            \
+  manageable(uintx, G1PeriodicGCSystemLoadThreshold, 0,                     \
+          "Maximum recent system wide system load as returned by the 1m "   \
+          "value of getloadavg() at which G1 triggers a periodic GC. A "    \
+          "load above this value cancels a given periodic GC. A value of "  \
+          "zero disables this check.")                                      \
 
 #endif // SHARE_VM_GC_G1_G1_GLOBALS_HPP
diff --git a/src/hotspot/share/gc/shared/gcCause.cpp b/src/hotspot/share/gc/shared/gcCause.cpp
index dd2dd8d600..6da2ff7ce9 100644
--- a/src/hotspot/share/gc/shared/gcCause.cpp
+++ b/src/hotspot/share/gc/shared/gcCause.cpp
@@ -103,6 +103,9 @@ const char* GCCause::to_string(GCCause::Cause cause) {
     case _g1_humongous_allocation:
       return "G1 Humongous Allocation";
 
+    case _g1_periodic_collection:
+      return "G1 Periodic Collection";
+
     case _dcmd_gc_run:
       return "Diagnostic Command";
 
diff --git a/src/hotspot/share/gc/shared/gcCause.hpp b/src/hotspot/share/gc/shared/gcCause.hpp
index 36b15d68d0..8de846ac80 100644
--- a/src/hotspot/share/gc/shared/gcCause.hpp
+++ b/src/hotspot/share/gc/shared/gcCause.hpp
@@ -76,6 +76,7 @@ class GCCause : public AllStatic {
 
     _g1_inc_collection_pause,
     _g1_humongous_allocation,
+    _g1_periodic_collection,
 
     _dcmd_gc_run,
 
diff --git a/src/hotspot/share/gc/shared/vmGCOperations.cpp b/src/hotspot/share/gc/shared/vmGCOperations.cpp
index b02305a6e1..f6b66064ec 100644
--- a/src/hotspot/share/gc/shared/vmGCOperations.cpp
+++ b/src/hotspot/share/gc/shared/vmGCOperations.cpp
@@ -81,7 +81,6 @@ bool VM_GC_Operation::skip_operation() const {
 }
 
 bool VM_GC_Operation::doit_prologue() {
-  assert(Thread::current()->is_Java_thread(), "just checking");
   assert(((_gc_cause != GCCause::_no_gc) &&
           (_gc_cause != GCCause::_no_cause_specified)), "Illegal GCCause");
 
@@ -110,7 +109,6 @@ bool VM_GC_Operation::doit_prologue() {
 
 
 void VM_GC_Operation::doit_epilogue() {
-  assert(Thread::current()->is_Java_thread(), "just checking");
   // Clean up old interpreter OopMap entries that were replaced
   // during the GC thread root traversal.
   OopMapCache::cleanup_old_entries();
diff --git a/src/hotspot/share/logging/logTag.hpp b/src/hotspot/share/logging/logTag.hpp
index 61bd169072..019ad5bdc7 100644
--- a/src/hotspot/share/logging/logTag.hpp
+++ b/src/hotspot/share/logging/logTag.hpp
@@ -119,6 +119,7 @@
   LOG_TAG(patch) \
   LOG_TAG(path) \
   LOG_TAG(perf) \
+  LOG_TAG(periodic) \
   LOG_TAG(phases) \
   LOG_TAG(plab) \
   LOG_TAG(preview)   /* Trace loading of preview feature types */ \
-- 
2.17.1

